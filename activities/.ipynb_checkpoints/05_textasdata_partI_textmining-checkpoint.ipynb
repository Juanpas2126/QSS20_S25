{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "## load packages \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "## nltk imports\n",
    "import nltk\n",
    "nltk.download()\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "## uncomment and download if this is your first \n",
    "## time running \n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "\n",
    "## sentiment analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "## specify to print all output in a call\n",
    "## and not just first\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## spacy --- if you get an error at the load step\n",
    "## need to download en_core_web_sm (google)\n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if working from within the repo, can use this relative path\n",
    "path_todata = \"../public_data/airbnb_text.csv\"\n",
    "\n",
    "## load data\n",
    "ab = pd.read_csv(path_todata)\n",
    "ab.head()\n",
    "ab.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual approach 1: look for a single word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using the `name_upper` var, look at where reviews mention cozy\n",
    "ab['is_cozy'] = ab.name_upper.str.contains(\"COZY\")\n",
    "\n",
    "## find the mean price by neighborhood and whether cozy\n",
    "mp = ab.groupby(['is_cozy', 'neighbourhood_group'])['price'].mean().reset_index()\n",
    "\n",
    "## reshape to wide format so that each borough is row\n",
    "## and one col is the mean price for listings that describe\n",
    "## the place as cozy; other col is mean price for listings\n",
    "## without that word\n",
    "mp_wide = pd.pivot_table(mp, index = ['neighbourhood_group'],\n",
    "                        columns = ['is_cozy'])\n",
    "\n",
    "mp_wide.columns = ['no_mention_cozy', 'mention_cozy']\n",
    "\n",
    "mp_wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual approach 2: score based on dictionary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COZY', 'LITTLE']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### construct dictionary\n",
    "space_indicators = {'small': ['COZY', 'COMFY', 'LITTLE', 'SMALL'],\n",
    "                   'large': ['SPACIOUS', 'LARGE', 'HUGE', 'GIANT']}\n",
    "\n",
    "\n",
    "## for each listing, find the number of occurrences\n",
    "## of words in each key\n",
    "\n",
    "### first, let's test with one listing\n",
    "practice_listing = \"NICE AND COZY LITTLE APT AVAILABLE\"\n",
    "\n",
    "### splitting that string at space and looking at overlap with each key\n",
    "### first, look at overlap with the list containing words for small\n",
    "words_overlap_small = [word \n",
    "                    for word in practice_listing.split(\" \") if \n",
    "                      word in space_indicators['small']]\n",
    "words_overlap_small\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### then, look at overlap with the list containing words for large\n",
    "words_overlap_large = [word for word in practice_listing.split(\" \") if \n",
    "                      word in space_indicators['large']]\n",
    "words_overlap_large\n",
    "\n",
    "### could then take length as a fraction of all words\n",
    "len(words_overlap_small)/len(practice_listing.split(\" \"))\n",
    "len(words_overlap_large)/len(practice_listing.split(\" \"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## specify example\n",
    "example_for_tag = \"This is a chill apt next to the subway in LES Chinatown\"\n",
    "example_for_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## try part of speech tagging using nltk\n",
    "tokens = word_tokenize(example_for_tag) # Generate list of tokens\n",
    "tokens_pos = pos_tag(tokens) # generate part of speech tags for those tokens\n",
    " \n",
    "## returns a list of tuples\n",
    "## first element in tuple is a word\n",
    "## second element in tuple is the part of speech\n",
    "#for one_tok in tokens_pos:\n",
    "    #print(one_tok)\n",
    "tokens_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assistant\n",
    "This error occurs because you're trying to use NLTK's tokenizer functionality, but you haven't downloaded the required data files for it to work.\n",
    "\n",
    "The error message indicates that the 'punkt_tab' resource is missing, which is needed for the word_tokenize function to work properly.\n",
    "\n",
    "Would you like me to provide the corrected code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User\n",
    "yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/juanrojas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/juanrojas/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/juanrojas/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m example_for_tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is an example sentence for part of speech tagging.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Now proceed with the tagging\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(example_for_tag)  \u001b[38;5;66;03m# Generate list of tokens\u001b[39;00m\n\u001b[1;32m     16\u001b[0m tokens_pos \u001b[38;5;241m=\u001b[39m pos_tag(tokens)  \u001b[38;5;66;03m# generate part of speech tags for those tokens\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/tokenize/punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[0;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/juanrojas/nltk_data'\n    - '/opt/anaconda3/nltk_data'\n    - '/opt/anaconda3/share/nltk_data'\n    - '/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Assistant\n",
    "# First, import nltk and download the required resources\n",
    "import nltk\n",
    "nltk.download('punkt')  # For word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')  # For pos_tag\n",
    "\n",
    "# Now import the specific functions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# Define your example text (which was missing in the original code)\n",
    "example_for_tag = \"This is an example sentence for part of speech tagging.\"\n",
    "\n",
    "# Now proceed with the tagging\n",
    "tokens = word_tokenize(example_for_tag)  # Generate list of tokens\n",
    "tokens_pos = pos_tag(tokens)  # generate part of speech tags for those tokens\n",
    " \n",
    "# Print the results\n",
    "for one_tok in tokens_pos:\n",
    "    print(one_tok)\n",
    "\n",
    "tokens_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens_pos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## use list iteration to extract proper nouns (NNP)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m## i'm first checking if the second element in the tuple\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m## is equal to NNP\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m## if so, i'm returning the first element in the tuple (the \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m## actual word)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m all_prop_noun \u001b[38;5;241m=\u001b[39m [one_tok[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m one_tok \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokens_pos\u001b[49m \n\u001b[1;32m      7\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m one_tok[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNNP\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m all_prop_noun\n\u001b[1;32m     10\u001b[0m all_adj_noun \u001b[38;5;241m=\u001b[39m [one_tok[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m one_tok \u001b[38;5;129;01min\u001b[39;00m tokens_pos \n\u001b[1;32m     11\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m one_tok[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJJ\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \n\u001b[1;32m     12\u001b[0m                one_tok[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNN\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokens_pos' is not defined"
     ]
    }
   ],
   "source": [
    "## use list iteration to extract proper nouns (NNP)\n",
    "## i'm first checking if the second element in the tuple\n",
    "## is equal to NNP\n",
    "## if so, i'm returning the first element in the tuple (the \n",
    "## actual word)\n",
    "all_prop_noun = [one_tok[0] for one_tok in tokens_pos \n",
    "                if one_tok[1] == \"NNP\"]\n",
    "all_prop_noun\n",
    "\n",
    "all_adj_noun = [one_tok[0] for one_tok in tokens_pos \n",
    "                if one_tok[1] == \"JJ\" or \n",
    "               one_tok[1] == \"NN\"]\n",
    "all_adj_noun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## modified from a real tweet\n",
    "\n",
    "## tweet\n",
    "d_tweet = \"\"\"We’ll be hosting on-campus COVID-19 booster clinics \n",
    "at Dartmouth College in New Hampshire from 9 a.m. to 6 p.m. on\n",
    "Monday, Jan. 10, and Tuesday, Jan. 11, at\n",
    "Alumni Hall in the Hopkins Center. For information on how to\n",
    "register and additional winter updates, head to\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_dtweet = nlp(d_tweet)\n",
    "print(type(spacy_dtweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## try a couple variations\n",
    "for one_tok in spacy_dtweet.ents:\n",
    "    print(\"Entity: \" + one_tok.text + \"; NER tag: \" + one_tok.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1\n",
    "\n",
    "Play around with different variations of the Dartmouth tweet and look at the results. For instance, try the following:\n",
    "\n",
    "- What happens if you abbreviate New Hampshire to NH?\n",
    "- What happens if you add the word Pfizer before COVID-19?\n",
    "- What entities seem misclassified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2\n",
    "\n",
    "How do we generalize from doing Named Entity Recognition one just one string to doing NER on a whole column that contains strings? By defining and executing a **function**, of course!\n",
    "\n",
    "Using the following sample of strings from airbnb listing names, define a function that takes in one airbnb string and does the following:\n",
    "\n",
    "- iterates over entities in that string (list comprehension would work well for this)\n",
    "- checks if each label indicates a place\n",
    "- returns the text of each original place/entity\n",
    "\n",
    "Then execute the function on the example strings through iteration (again, list comprehension works well here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for runtime purposes, take a sample of the airbnb listing names\n",
    "ab_name_examples = ab.name[10:15]\n",
    "ab_name_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your function code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the default scorer on a few example phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize a scorer\n",
    "sent_obj = SentimentIntensityAnalyzer()\n",
    "print(type(sent_obj))\n",
    "## score one listing\n",
    "practice_listing = \"NICE AND COZY LITTLE APT AVAILABLE\"\n",
    "sentiment_example = sent_obj.polarity_scores(practice_listing)\n",
    "sentiment_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding phrase with word terrible and score\n",
    "practice_listing_2 = \"NICE AND COZY LITTLE APT AVAILABLE. REALLY TERRIBLE VIEW.\"\n",
    "sentiment_example_2 = sent_obj.polarity_scores(practice_listing_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding phrase about rats; bad but might not be in scoring dictionary\n",
    "practice_listing_3 = \"NICE AND COZY LITTLE APT AVAILABLE. HAS RATS THOUGH.\"\n",
    "sentiment_example_3 = sent_obj.polarity_scores(practice_listing_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## summarize all 3\n",
    "print(\"String: \" + practice_listing + \" scored as:\\n\" + str(sentiment_example))\n",
    "print(\"String: \" + practice_listing_2 + \" scored as:\\n\" + str(sentiment_example_2))\n",
    "print(\"String: \" + practice_listing_3 + \" scored as:\\n\" + str(sentiment_example_3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the dictionary with manually-added words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(sent_obj.lexicon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lexicon is a dictionary where the key\n",
    "## is the word\n",
    "## the value is the score (negative = negative)\n",
    "## here, i'm benchmarking the negativity of the\n",
    "## rodents to the negativity of the word aversion\n",
    "sent_obj.lexicon['aversion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a dictionary with \n",
    "## negative scores for pests\n",
    "pest_words = {\n",
    "    'rat': -1.9,\n",
    "    'rats': -1.9,\n",
    "    'mice': -1.9,\n",
    "    'mouse': -1.9,\n",
    "    'roach': -1.9,\n",
    "    'cockroach': -1.9\n",
    "}\n",
    "\n",
    "\n",
    "## initiate new sentiment object\n",
    "## so that we don't alter old one\n",
    "## use.update to add new words\n",
    "new_si = SentimentIntensityAnalyzer()\n",
    "new_si.lexicon.update(pest_words)\n",
    "\n",
    "## try re-scoring the third example\n",
    "## see negative\n",
    "print(\"After lexicon update: \" + practice_listing_3 + \" scored as:\\n\" + \\\n",
    "      str(new_si.polarity_scores(practice_listing_3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
